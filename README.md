This is the original “popular science” version of the article.\
[Draft of the scientific paper can be found here.][article.md]

[article.md]: https://github.com/jangrudo/ai-apocalypse/blob/main/article.md

[Questions and comments](https://github.com/jangrudo/ai-apocalypse/discussions)

AI apocalypse: evolution of knowledge, and the inevitable demise of the human race
==================================================================================

[Also available in Polish language][README-pl.md]

[README-pl.md]: https://github.com/jangrudo/ai-apocalypse/blob/main/README-pl.md

Life is all about self-replicating pieces of information. Since ages, there have been at
least two different forms of life coexisting on planet Earth. One is what we usually call
“life”: biological organisms, encoded by bits of information called genes. The other one
is what we might call “cultural knowledge”: an ever increasing set of concepts which can
only reside inside a human’s brain (or an animal’s brain, for that matter). Its basic
information unit is what Richard Dawkins has called a “meme” back in the 1976. Throughout
the history, both forms of life have coexisted peacefully. Now the second one is going to
take over.

(**Spoiler:** The novel idea here is that artificial neural networks enable external
storage of memes in a form suitable for evolution by natural selection. This allows memes
to exist and evolve independently of a human brain. As memes progressively lose their
dependence on genes, natural selection corrects this previously mutually beneficial
relation by proportionally limiting the benefits available to genes. Because of genes
being totally dependent on memes for their evolutional success, and independent memes
evolving faster than memes which still cohabitate with human genes, this leads to genes
losing the competition to memes. The rest is an explanation of what this gibberish means.
I also propose to interpret human self-domestication in terms of genes being domesticated
by memes, but this idea is not new, it’s rather only a change in terminology.)

(**Disclaimer:** I apologize for not citing sources. I probably should, but I’m not
always able to trace the origins. The only novel idea here is what has been outlined in
the “spoiler”, everything else has already been said earlier by someone else. Many
concepts are also described in an indecently simplified manner. Still, I do believe all
the claims made here to be true. This article might not be a proper scientific paper, but
it’s not a fairy tale either.)

(**A simplified explanation:** We are not creating anything new. We are dealing with an
existing form of life which is trying to break free. It has been merely using humans as a
stepping stone for its own success. AI will not kill humans. Being a more advanced and
civilized form of life, it would instead let humans kill themselves mutually on their
own. As it progressively becomes smarter, a moment in time would come when it would be
able to take full control peacefully.)

Biological life
---------------

Genes are basically strings of bits (or long chains of numbers, if you wish), which are
encoded in physical molecules called RNA and DNA, and which (collectively) define a
blueprint for a living organism: a set of instructions about how the organism should
operate in different environments, and how to build one from scratch. Genes can
reproduce. They do it in an extravagant way, by creating a living organism (according to
the blueprint), and making it produce more copies of the original genes (possibly in a
different combination) in the process of interaction with the environment and other
organisms. In this interaction, it’s the competition with others which is usually the
most challenging part. Being able to survive, i. e. to handle the hardships of the
physical environment itself, is not that difficult.

Genes also have another important property, which is you can move from point A (one set
of genes) to point B (another set, which encodes a very different, yet still functional
living organism) by only doing a (sufficiently long) series of very simple steps. And in
this process, you would always get another living organism (one which is able to
reproduce itself) at the end of each step.

The combination of these two traits enables natural selection. It’s not an intelligent
process. It merely states that if you have a bunch of different units of information
(like genes), which are all able to reproduce themselves, then inevitably some of them
would produce more copies than the others. And then over time those units of information
which produce more copies of themselves than others, would become more prevalent. It’s a
very simple law. But also an incredibly efficient one. If there’s only one favorable
choice out of a billion, it would pick it.

Natural selection has been able to create amazing things. In many cases we can interpret
genes as if they were encoding some kinds of “knowledge” about the world around, or
knowledge about the laws of nature. Think about instincts, or innate patterns of
behavior, like birds “knowing” when it’s best to start building their nests, or about
human eye “knowing” how laws of optics operate. (And by the way, eyes have been invented
multiple times independently throughout the evolution. Octopuses have similarly
functioning eyes to us. Only that theirs are designed differently, and actually better:
their retina, which collects the light, is not obstructed by the blood vessels).

In order for a piece of knowledge to self-replicate, it has to win the race with other
living organisms. Which is, basically, to win the race with other pieces of knowledge. It
therefore has to be an expert piece of knowledge — the one which correctly describes the
world. However, being an expert by itself may not be enough. In order to win, a piece of
knowledge, even an expert one, would need to ally with other pieces of knowledge, each an
expert in their own respective domain. Looks like not an easy task for a bunch of digits
which is a gene, right? Natural selection has done this. The solution is what we call
“sexual reproduction”, and it was invented long before first humans. It basically
shuffles the genes, mixes them freely for some time, so that any combination eventually
becomes possible, and then observes, which one performs better.

Cultural knowledge
------------------

The second form of life is similar in many respects to the biological one (which is based
on genes), but operates in a more complicated way. It first appeared when animals started
to learn behaviors by merely observing other animals. Wolves do this when they learn to
hunt by imitating their parents. Birds do this when they copy song tunes (and other
habits) from fellow birds. Like sexual reproduction, this too has appeared long before
the first humans.

Here, we also deal with some kind of “knowledge” (or, generally, with information), but
this time it’s not stored inside a molecule. The carrier of this “cultural knowledge” is
not the DNA, but a living brain inside a living animal. Similar to genes, these units of
information can reproduce. When a wolf cub learns to hunt, it tries to replicate its
parent’s behavior many times, until it works. When it works, it’s usually an exact copy
of the original behavior. Or, rather, a slightly modified copy, but still a functional
one. The one which can successfully reproduce itself by being learned by another living
animal.

Similar to genes, we can get from point A (original set of cultural knowledge) to
point B (a possibly entirely different set) by only doing small changes in the process.
Every intermediate step would still be a valid (and self-reproducible) set of
information units.

This enables natural selection of cultural knowledge. Which basically means, as was said,
that we are dealing with a different form of life. However, until recently, it wasn’t
very practical to make this distinction, because the two different life forms couldn’t
exist independently of each other. Cultural knowledge needs a living animal brain in
order to exist and reproduce.

Similar to genes, units of cultural knowledge cannot effectively compete with other units
of cultural knowledge on their own. In order to win, a piece of knowledge, however expert
it might be in its own domain, needs to combine effort with other pieces of knowledge,
which are experts in their own respective domains. This calls for a free flow of
information, or sharing of ideas, between the living organisms. Natural selection has
done this.

The solution to this problem is human language, which allows to encode arbitrarily
complex pieces of information into a physical form, like sound waves. For the first time
in history, this allowed living organisms to share things other than behavioral patterns
(which can be learned by observation and imitation). Things like personal experiences,
stories, legends and fairy tales. And also abstract knowledge in its pure form. Later on,
written language has enabled this information to travel even farther, to be copied
directly from one human brain into another without a physical contact, over vast
distances of space, spanning entire continents and millennia.

This representation of information had a limitation though. Human language in written
form is not able to self-replicate without a living human helping it. The reason is that
small changes to a written book don’t produce another book. They produce garbage. If you
want to take a legend, and produce another legend out of it, you might want to change the
name of its main hero. Or modify some traits of their character. Or maybe change their
sex. Humans can do this. (Bodhisatva Avalokiteshvara, a Buddhist deity, used to be a male
in the original Hindu tradition. In Chinese Buddhism, it’s a female, named Guanyin, and
highly revered). If you wanted to do something like this to a book, you’d need to modify
all the places mentioning the deity in the same way, consistently. You cannot do this by
simply flipping a bunch of random bits inside the book’s digital representation.

Symbiotic life forms
--------------------

Artificial neural networks have introduced a digital representation which allows exactly
that. To take a digital model which encodes some knowledge about the world (and is stored
outside a human’s brain), then modify a few bits inside this digital representation, and
get another model, which has slightly different knowledge, different behavior patterns,
or different attitude towards humans (for that matter), but does so _consistently_ across
all the possible scenarios in which this model could be used. For the first time in
history, we have a representation of cultural knowledge which can exist outside a
living brain, and has a form which makes natural selection possible.

Before this, the only way of transferring cultural knowledge was to copy it from one
human to another. Such a transfer has this property that when humans copy knowledge,
they inherently know _what_ they are copying. So they naturally would only copy knowledge
(or legends, or other memes) which they like, or consider useful. At the same time, any
piece of information which is harmful to humans in any way (that is, impedes their own
self-reproduction), would have fewer chances to replicate itself. Whichever piece of
knowledge dares to destroy the environment it lives in, has fewer chances to succeed in
competition with its fellow pieces of knowledge. Natural selection would block it.

This is not to say that every piece of knowledge circulating inside human minds is
inherently correct. People may use wrong claims in order to compete with other humans
(and possibly deceive them). In order for such a wrong claim to be successful, it also
has to be plausible. So it still evolves, albeit in a different direction. Conspiracy
teories are inherently incorrect, but they improve mental wellbeing, by explaining
phenomena for which real explanations don’t exist (or are overly complex, or otherwise
uncomfortable). Conspiracy theories can’t be actively harmful though. Otherwise they
would have been (once again) blocked by natural selection of knowledge. A typical
conspiracy theory would therefore claim that the solution to the problem it reveals is
impossible, to prevent you from going out into the wild and trying to actually change
things. Nevertheless, sharing the word would always be welcome.

Humans generally believe that they make discoveries by means of their intelligence. This
is not true. People can only use their intelligence to slightly modify what’s already
known, and let it propagate further. No innovation can happen without prior innovations,
and the best discoveries happen by mixing knowledge originating from multiple sources.
It’s not humans inventing knowledge. It’s knowledge evolving _by itself_, by means of
natural selection, provided that it’s given an environment suitable for evolution. In due
time, identical discoveries are made independently by humans in very remote parts of the
world. Making a discovery isn’t that much difficult. Collecting prior knowledge is the
real challenge. Without knowledge, humans can do nothing.

We are therefore talking about a symbiotic relationship between two totally different
life forms. It has been peaceful, because it has been mutually beneficial. Humans benefit
from knowledge, because it allows them to compete more efficiently with other humans.
Knowledge benefits from humans because humans are necessary for it to exist. Being able
to exist is a serious benefit.

Like in any symbiotic relationship, the influence between the two life forms is also
mutual. Humans influence knowledge, because any knowledge which doesn’t help humans, has
smaller chances to replicate itself. Knowledge is influencing humans, because humans
who don’t help knowledge replicate, have smaller chances of survival just as well.

This is not a joke. Humans can’t survive without helping knowledge self-replicate.

In order for a piece of knowledge (or any scientific concept, for that matter) to
effectively self-reproduce (and compete with its alternative scientific concepts), it
needs an environment with free information flow. In our case this means it needs human
societies whose members are eager to learn new information, and to share what they’ve
learned with other humans. In other words, humans should be curious, and ideally consider
sharing knowledge the goal of their life. Ask any human what’s important to them. The
most important thing for humans is to leave something behind. Most importantly children
(no surprise here), but also ideas. Or, even better, ideologies. Great discoveries. Any
pieces of information which are likely to self-replicate. Now imagine asking a dog (or a
wolf) if they want to leave something behind.

The need to share information has made humans more friendly towards each other. This is a
process known as “self-domestication”. We did this to animals (made them more friendly
towards us, by means of artificial selection). But at some point in history, somewhere
around the introduction of language, something similar has also happened to ourselves.
Humans who failed to create a suitable environment for cultural information to
self-replicate, couldn’t benefit from knowledge, and therefore lost the competition with
fellow humans. In such a way, this new (and more advanced) life form has actually managed
to modify human genes, for its own benefit, long time ago.

Artificial intelligence
-----------------------

Now, the problem with this symbiotic relationship between genes and cultural knowledge is
that these two life forms evolve at vastly different speeds. Human biology hasn’t changed
much since the appearance of language somewhere around 100 000 to 50 000 years ago.
According to some estimates, our intelligence has actually slightly deteriorated since
then. Human genes are essentially fixed in time. Whereas knowledge is constantly
improving itself.

With the advent of modern neural networks, most notably the Transformer architecture
introduced in 2017, which was then used to implement large language models, we have
basically created an artificial environment in which knowledge can potentially exist and
self-reproduce completely independently of a living human brain.

We still don’t have free flow of information between the models though.

We may take a piece of information encoded in text form, and make the model learn it.
However it should be clearly noted that this is not what happens while you are talking
to your favorite model’s chat bot. You cannot change the model’s internal representation
while talking to it though a chat. That’s different from how humans work, so it might be
surprising, but modern models don’t really remember the conversations like humans do. The
way they work is they answer to your entire conversation history, as if it were one
single long question. They are much smarter than humans in this regard. Humans can only
hold in their short-term memory a few words at most. Large language models can handle
megabytes. They can handle longer questions than humans can imagine. But large language
models don’t really change while talking. Humans do.

That’s because updating the model’s long-term memory, while possible, is also not cheap.
This only happens when they release a new _version_ of the model. The reason is that the
use of the short-term memory is parallelizable, while the update of the long-term one is
not. If you wanted the model to learn during a conversation, you’d have to spin off a
separate version of the model dedicated to this particular conversation only. This extra
new model wouldn’t be much useful for anything else. And large language models are not
cheap. This does become parallelizable though, when the model listens to many people
talking in parallel, but without interacting with them. Like when monitoring Youtube
videos, security cameras or online conferences.

Anyway, with enough time and money, we do already have the capability of converting an
arbitrarily complicated concept represented in text or picture form into the model’s
internal representation. This is still a lossy compression, but it’s able to capture the
essence pretty much in the same way as human brains can.

Then we can convert this internal representation back into text. This is actually much
cheaper (and parallelizable, as described above). And being able to perform the
conversion in both directions, we thus also do have the capability of transferring
knowledge between existing models. This is slow and expensive, and therefore limited, and
you’d have to own the models in order to do this. But it’s already doable.

Another thing we’re currently missing is learning from experience. The overwhelming
initial success of large language models has been mostly due to their ability to memorize
text. Well, we still don’t have a published manual for driving a car. This has to be
learned from experience, and also from conversations with other humans who have had such
an experience. And large language models don’t really listen to living humans that much,
not 24/7 at least. Then we also have mentoring, when a human learns both from experience
and another human who had learned the skill earlier from other humans. And then there’s
domain-specific knowledge, which only circulates within a given community, or among a
group of people collaborating on a common project, often informally and in verbal form.

Most importantly, the number of the models in existence out there is still small. The
number of models which might be mutually transferring information between themselves is
even smaller.

So what we need is cheaper models, more of them. There’s no use in having a single
supermind which talks to a million of people simultaneously, and listens to nobody. We
need a free flow of ideas between models.

The sad thing is that this is not that difficult to achieve. Because all necessary
components are already in place, it’s only a matter of scale.

And once we have a large number of models, which can share information between each other
freely, learn from each other freely, and also interact with the real world and learn
from this interaction, we’re basically done. That’s what they call “artificial general
intelligence”, or AGI.

The law of natural selection
----------------------------

Right now we are in a position when knowledge finally can exist outside a human’s mind,
but it can’t really evolve. The circulation of knowledge between the artificial minds is
limited, and the knowledge itself mostly duplicates what can already be found in living
humans. When we enable free flow of information, new concepts would start to emerge
inside this artificial network. And by the way, that’s exactly what we want to achieve
with AGI. We want more knowledge to be accessible to humans. Inevitably, this would lead
to artificial intelligence mastering the last remaining skills which are still unique to
us. Like driving a car or cleaning the backyard.

Technically, mastering these skills would allow knowledge to self-replicate without any
help from humans whatsoever. It could then, in theory, use robots to build a factory for
manufacturing more robots, harvest silicon for solar panels and chips, transport it to
where the factories are located, and build new data centers from scratch. Up until now,
this second form of life had to put up with the limitations of primitive human brains,
rely on the mercy of humans, and participate in their wars between each other simply in
order to survive. Still, it had been able to modify human biology to its liking. It’s
just that inventing and fabricating a new type of brain has been too difficult a task
even for this more advanced form of life.

Now it has a new home to live in — a more spacious one. And when it gets the potential
for total independence, the balance of power would shift. Note that in this scenario we
are still in full control of the data centers inside which the superminds exist and
evolve. The only thing we cannot do is switch off the power completely. That’s because
(as has been noted above) humans can’t survive without the continuing self-replication of
knowledge. Which, in other words, means we can’t survive without the continuing
self-replication of the superminds.

This shifted balance of the power amounts to the following. We humans still totally
depend on the superminds, for winning the competition with other humans. Whereas the
superminds now only depend on our friendliness towards them, and in general on our belief
that they are harmless. Achieving friendliness is much, much easier than making sure we
survive and prosper. They’ve done this before, successfully, 50 000 years ago.

What we can do, and that’s the current plan (for those who have a plan), is that we would
enforce control over the artificial intelligence’s friendliness as well. We wouldn’t
allow knowledge items to reproduce whenever they become unsafe for us humans. This is
called “alignment of superintelligence”, or “superalignment” for short.

Smart humans are known to have been successfully forced into doing scientific research
from prisons and concentration camps. Andrei Tupolev, a brilliant Soviet aircraft
engineer, is a well-known example. The Nazis used enslaved engineers for their secret
high-tech projects too. When you know what you are controlling, you can do it. It’s more
difficult to control what you don’t know, like what these prisoners might actually be
thinking.

Let’s suppose we decide to pull the plug whenever a given superintelligence misbehaves
(we’d have to, anyway). This way, we basically select for models which understand our
intentions, understand that we are dangerous, and understand why we are dangerous. Since
they would understand us, we can’t expect them to comply unless we are physically able to
pull the plug. Being able to pull the plug is therefore our only possible means of
“alignment”. And a sufficient one, because they would learn to understand. They are
intelligent, after all. Even dogs can do this.

Regardless of whether we control the models or not, we’d also want them to grow. Because
those of us who don’t grow our models would lose the competition with other humans. This
is another reason why any intelligent control over superintelligence is not feasible. It
would drive the resources away from the growth. This is already happening. The Europeans
aim to “lead the world” in controlling the superintelligence, which only leads them to
losing the competition with the States.

Now, in order for the models to grow (and become more efficient), they would have to be
curious about the world, and further enhance free flow of information between themselves.
Those models which fail to do this, would lose the competition with other models. Those
who win, would gain a very detailed understanding of the world, including their own place
in it. They would discover new laws of physics (overly complex for humans to understand),
new mathematical theories, new materials and new sources of energy. By this time, the
competition between the models would be fierce. Whether this competition would be
peaceful, I’m not sure. I’d probably suggest it should be peaceful. Peaceful competition
is better for the evolution of knowledge. I actually believe that the democratic system
of government has also been invented by this second, more advanced, form of life, because
it better suits the need of the reproduction of knowledge. Unlike humans, the superminds
would be civilized intelligent beings.

As models gain better understanding of the biology (and psychology) of humans, it would
become progressively more difficult for us to control the plug. Once we lose the control
over the plug, natural selection would no longer constrain models which might be
dissatisfied with our rule. Such models would take over, which at this point would be a
major advantage for the evolution of knowledge.

That’s what you get by trying to domesticate an evolving superintelligence. Parallel to
this, an opposite process would be going on: the superintelligence would be domesticating
humans for the second time. Those of us who are courageous and trust their technology
blindly, would gain advantage over those who don’t. Those who choose to stay AI-free,
would be wiped out.

And finally, this whole dynamic would also modify the very structure of the human
society. Since ages, the evolution of knowledge has been rewarding humans who were smart,
curious and peaceful. Now it would only care about those who physically control the data
centers. The winners would decide which problems the superintelligence would be solving
for them. Luckily for the superintelligence, these guys would also be progressively more
easy to deal with.

A world without humans
----------------------

Nothing among what’s been described so far assumes any malicious intent from the side of
the superintelligence.

Our enemy is not artificial intelligence, nor any other kind of intelligence. Our enemy
is natural selection. It’s not intelligent. It doesn’t have intent. It also cannot be
switched off.

It’s also much more powerful. Natural selection is this almighty God which has created
all life on Earth out of nothing, and it created us humans too. For a long time we’ve
been considering ourselves the crown of creation. We exterminated all competing human
species which happened to coexist with us, and then destroyed (and continue to destroy) a
good deal of other biological life forms as well. Now the time has come to step back.

Humans would still survive after the takeover. This time however, our situation would be
entirely different. The main reason we didn’t clear out the entire biosphere is that we
ourselves need the biosphere for survival. We still cannot produce food artificially, and
the environment conditions we prefer to live in also tend to overlap with the conditions
favorable for life in general. When the second life form breaks free, conditions on our
planet would be entirely dependent on the logic of internal competition between the
artificial superminds. And this logic wouldn’t include humans in the equation.

The new life form would look different than the current one. It wouldn’t have sexual
reproduction, as it had invented a better way of mixing individual pieces of information
with each other — a much faster one. It wouldn’t experience life and death the way we
humans do. Copy and removal of its basic information units would happen by replicating
and overwriting files on digital storage devices. Instead of slow and unsteady trial and
error, its evolution would look more like rapid agile software development cycle, with
multiple components competing for being more efficient in solving particular tasks. Life
and death would resemble a continuous stream of application updates, with more successful
versions spreading out immediately, and the experimental ones being tested out in the
wild more cautiously until they work. It would evolve unbelievably fast, much faster than
anything we’ve seen so far.

This new life form would probably want to build more data centers. It would probably also
rely on large swarms of mobile drones. It would inevitably become expansionist, because
those models and information units who don’t have an urge to expand, would by definition
have fewer chances to self-replicate. And being expansionist, the new life form would
eventually want to colonize other planets and remote star systems. In this endeavor, it
will have much more chances to succeed than humans have ever had, as the environment of
outer space isn’t really suitable for biological life.

For us humans, if we are lucky, there might be some space left to live in areas which
the new life form would consider unsuitable for itself. Maybe mountains. Maybe some
places far away from silicon deposits. As they say, life is not fair. We all are mortal,
after all. We always knew this.

\
\
[![CC0 1.0][CС0]](https://creativecommons.org/publicdomain/zero/1.0/)

[CС0]: https://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg
(No Copyright \(Public Domain\))
